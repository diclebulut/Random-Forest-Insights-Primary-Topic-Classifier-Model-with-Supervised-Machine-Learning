{"cells":[{"source":"# Insights Primary Topic Predictor \n\n![datascience-hero](datascience-hero.jpeg)\n\nInsights Primary Topic Predictor is written to predict the “primary topic” category which an anecdote falls into using natural language processing and sentence vectors (with a list of 2 million words and their coordinates in a 300 dimensional space where coordinates are determined by the relations of words among each other). It is a Python script written in IDLE version 3.10.4.\n\nThis predictor takes CBI Insights Platform exports in excel format as input. The output of this predictor is in two categories: model properties (the cross validation results of the model to determine overfitting, parameter training results to fine tune the model’s prediction) and predictions (an excel document that has additional columns to display post-natural-language processing version of the data points, vector coordinates, actual primary topics of the anecdotes given by CBI staff and predictor’s assigned topic for the anecdote. \n\n\n","metadata":{},"id":"93024ba2","cell_type":"markdown"},{"source":"## Disclaimer\n\nThe data needed for this script to work is not available due to organisational confidentiality reasons. Therefore, the reader must review the code with hypothetical data with stated specifications in mind.","metadata":{},"cell_type":"markdown","id":"b2c088d3-a959-46d9-9073-81725c44f76c"},{"source":"## Methodology and Script\n\n### Functions as they are used in the script\n\nThis script uses a total of 9 functions. We can inspect them in 3 categories: building sentence vectors, training and evaluating the model and displaying the output in excel format.","metadata":{},"cell_type":"markdown","id":"4f7757c7-326b-4ff9-b934-53e5059ff216"},{"source":"#### Introduction and Preperation","metadata":{},"cell_type":"markdown","id":"ad682fcf-b2e2-4237-9630-68b986dd4b5a"},{"source":"##### Packages and Libraries\n\n- numpy\n- pandas\n- time\n- spacy\n- nltk\n- “stopwords” from “nltk.corpus”, \n- random\n- “RandomForestClassifier” from “sklearn.ensemble”\n- “train_test_split”, “KFold” and “StratifiedKFold” from “sklearn.model_selection”","metadata":{},"cell_type":"markdown","id":"70ac7991-61e5-407a-acf4-4b83acaaea92"},{"source":"import numpy as np\nimport pandas as pd\nimport time\nimport spacy\nfrom nltk.corpus import stopwords\nimport random\nimport nltk\n\n#machine learning classifier, classifies the sentence vector to numbers 0-4 (topics)\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n\n# requirements.txt\n# numpy\n# pandas\n# openpyxl\n# scikit-learn\n# spacy\n# nltk\n\n#console (to be executed in the console)\n# python -m spacy download en_core_web_sm\n#in python (to be executed in the text editor)\n# nltk.download('stopwords')\n\nnlp = spacy.load('en_core_web_sm')\n\n# Don't take the default, e.g. shouldn't ignore the word \"people\"\n# Also, some rows have no data, should exclude from analysis\nstop_words = set([w.lower() for w in stopwords.words()])\nstop_words.remove('people')\n\n\nglobal df, wordVecs\n\n#trying to read df throws error\n#df = pd.read_excel(r'C:\\\\Users\\\\username\\\\Downloads\\\\training02.xlsx')\n\n# Could possibly store efficiently with numpy\n# e.g. 1 file - list of 2M words, other file 2M x 300 array\n","metadata":{},"cell_type":"code","id":"e061036a-ed15-48a9-971a-c92ae549cd7d","execution_count":null,"outputs":[]},{"source":"\n\n#### Building Sentence Vectors\n\n- loadWordVectors()\nLoads the 2 million x 300 array named “lexvec.commoncrawl.300d.W.pos.neg3.vectors” which should be downloaded separately\n\n- buildSentenceVectors()\nThis is where we input the CBI insights export in excel format. It loads word vectors from the previous function. Then splits the sentences into useful bits by removing stop words. Finally, it fits the individual words from data points into that 300 dimension word vector and saves the binary dataframe with each data point’s coordinates in that array (lexvec.commoncrawl.300d.W.pos.neg3.vectors).\n\nThis function should be repeated anytime the user makes a change on the excel document (e.g. adding new datapoints or a new export) and on the stop words.\n\nAny other time, this function should stay as comment because it takes a while for it to run and its output (binary data frame) is already saved into a pickle.\n\n- splitSentence(s)\nSplits the data points into useful words and removes stop words (stop words don’t include “people” and other relevant words can be excluded from stop words depending on the context). This function is included in “buildSentenceVectors()” function.\nsentenceVector(words)\nThis function takes the average (of coordinates) of the words from data points that appear in the word vector database. It returns \"No words recognised from sentence, check stop list” if the datapoint has no words left after stop word cleaning.\n\n“Tot” is a 300 length vector of 0s initially. The function adds the word vector for each word in the data point, then divide by the count. Tot/count is the average word vector of the words in the data point. Therefore it assigns a single word vector for the whole data point. \n\n- loadSentenceVecs():\nThis function read the precomputed sentence vectors for practicality.\n\n- rowsWithoutAnyWords():\nThis function detects empty rows.\n\n","metadata":{},"cell_type":"markdown","id":"ff025b60-3109-4405-aab0-63c348b3a28f"},{"source":"def loadWordVectors():\n    global wordVecs\n    wordVecs = {}\n    with open('C:\\\\Users\\\\username\\\\Downloads\\\\lexvec.commoncrawl.300d.W.pos.neg3.vectors', encoding=\"UTF-8\") as f:\n        print(f.readline().strip())\n        i = 1\n        t0 = time.time()\n        line = f.readline()\n        while len(line) > 0:\n            line = line.strip()\n            split = line.split(\" \")\n            wordVecs[split[0]] = np.array([float(x) for x in split[1:]])\n            if i % 100000 == 0:\n                t1 = time.time()\n                print(i, split[0], t1-t0)\n                t0 = t1\n            line = f.readline()\n            i += 1\n\n    print('word vecs loaded from file')\n\n    return wordVecs\n\ndef buildSentenceVectors():\n    global df, wordVecs\n    df = pd.read_excel(r'C:\\\\Users\\\\username\\\\Downloads\\\\Latest2108.xlsx')\n    print(\"Data frame loaded\")\n  \n\n    # f = open('C:\\\\Users\\\\username\\\\Downloads\\\\lexvec.commoncrawl.300d.W.pos.neg3.vectors')\n    # 2000000 300\n    # the 0.008314 0.026552 ...\n\n    wordVecs = loadWordVectors()\n    print(\"Word vectors loaded\")\n\n    # TODO: No punctuation e.g. need to map \"they're\" to \"they are\"\n    # hyphens being replaced with \\x ?\n    # need to remove punctuation e.g. full stop at end of sentences\n\n    df['words'] = df['Data Point'].map(splitSentence)\n    print(\"Words separated\")\n    df['wordVec'] = df['words'].map(sentenceVector)\n    print(\"Complete!\")\n\n    # save/load binary dataframe\n    df.to_pickle('C:\\\\Users\\\\username\\\\Downloads\\\\trainingWithVecs2')\n    df = pd.read_pickle('C:\\\\Users\\\\username\\\\Downloads\\\\trainingWithVecs2')\n    return df\n\n\n#function that gets useful words from sentences\n#nlp natural language processing library\ndef splitSentence(s):\n    if type(s) != str:\n        return []\n    return list(filter(lambda s : s.isalpha() and s not in stop_words, [token.text for token in nlp(s.lower())]))\n\n# average of those words that appear in the word vector database\ndef sentenceVector(words):\n    tot = np.zeros(300) # dimension of vectors\n    count = 0\n    for word in words:\n        if word in wordVecs:\n            tot += wordVecs[word]\n            count += 1\n    if count == 0:\n        print(\"No words recognised from sentence, check stop list\")\n        return tot\n    return tot/count\n\ndef loadSentenceVecs():\n    print('Loading word vectors from pre-computed file')\n    return pd.read_pickle('C:\\\\Users\\\\username\\\\Downloads\\\\trainingWithVecs2')\n\n# buildSentenceVectors for rebuilding vectors, loadSentenceVecs to just reload from a file\n#df = buildSentenceVectors()\ndf = loadSentenceVecs()\n\n# analysis functions to investigate better approaches\ndef rowsWithoutAnyWords():\n    return df[df['words'].isin([[]])]\n","metadata":{"executionTime":536,"lastSuccessfullyExecutedCode":"def loadWordVectors():\n    global wordVecs\n    wordVecs = {}\n    with open('C:\\\\Users\\\\dbulut\\\\Downloads\\\\lexvec.commoncrawl.300d.W.pos.neg3.vectors', encoding=\"UTF-8\") as f:\n        print(f.readline().strip())\n        i = 1\n        t0 = time.time()\n        line = f.readline()\n        while len(line) > 0:\n            line = line.strip()\n            split = line.split(\" \")\n            wordVecs[split[0]] = np.array([float(x) for x in split[1:]])\n            if i % 100000 == 0:\n                t1 = time.time()\n                print(i, split[0], t1-t0)\n                t0 = t1\n            line = f.readline()\n            i += 1\n\n    print('word vecs loaded from file')\n\n    return wordVecs\n\ndef buildSentenceVectors():\n    global df, wordVecs\n    df = pd.read_excel(r'C:\\\\Users\\\\dbulut\\\\Downloads\\\\Latest2108.xlsx')\n    print(\"Data frame loaded\")\n  \n\n    # f = open('C:\\\\Users\\\\username\\\\Downloads\\\\lexvec.commoncrawl.300d.W.pos.neg3.vectors')\n    # 2000000 300\n    # the 0.008314 0.026552 ...\n\n    wordVecs = loadWordVectors()\n    print(\"Word vectors loaded\")\n\n    # TODO: No punctuation e.g. need to map \"they're\" to \"they are\"\n    # hyphens being replaced with \\x ?\n    # need to remove punctuation e.g. full stop at end of sentences\n\n    df['words'] = df['Data Point'].map(splitSentence)\n    print(\"Words separated\")\n    df['wordVec'] = df['words'].map(sentenceVector)\n    print(\"Complete!\")\n\n    # save/load binary dataframe\n    df.to_pickle('C:\\\\Users\\\\dbulut\\\\Downloads\\\\trainingWithVecs2')\n    df = pd.read_pickle('C:\\\\Users\\\\dbulut\\\\Downloads\\\\trainingWithVecs2')\n    return df\n\n\n#function that gets useful words from sentences\n#nlp natural language processing library\ndef splitSentence(s):\n    if type(s) != str:\n        return []\n    return list(filter(lambda s : s.isalpha() and s not in stop_words, [token.text for token in nlp(s.lower())]))\n\n# average of those words that appear in the word vector database\ndef sentenceVector(words):\n    tot = np.zeros(300) # dimension of vectors\n    count = 0\n    for word in words:\n        if word in wordVecs:\n            tot += wordVecs[word]\n            count += 1\n    if count == 0:\n        print(\"No words recognised from sentence, check stop list\")\n        return tot\n    return tot/count\n\ndef loadSentenceVecs():\n    print('Loading word vectors from pre-computed file')\n    return pd.read_pickle('C:\\\\Users\\\\dbulut\\\\Downloads\\\\trainingWithVecs2')\n\n# buildSentenceVectors for rebuilding vectors, loadSentenceVecs to just reload from a file\n#df = buildSentenceVectors()\ndf = loadSentenceVecs()\n\n# analysis functions to investigate better approaches\ndef rowsWithoutAnyWords():\n    return df[df['words'].isin([[]])]\n","collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"256025cb-825b-487d-b55f-abfd0085a1fb","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Loading word vectors from pre-computed file\n"},{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdbulut\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtrainingWithVecs2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# buildSentenceVectors for rebuilding vectors, loadSentenceVecs to just reload from a file\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m#df = buildSentenceVectors()\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mloadSentenceVecs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# analysis functions to investigate better approaches\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrowsWithoutAnyWords\u001b[39m():\n","Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mloadSentenceVecs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadSentenceVecs\u001b[39m():\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading word vectors from pre-computed file\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdbulut\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtrainingWithVecs2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"],"ename":"NameError","evalue":"name 'pd' is not defined"}]},{"source":"#### Training and evaluating the model \n\n- cross_validate(df)\nThis function uses “StratifiedKFold” to split the data points into n and then to use this split to train and test the model on different fractions of the data. Default split is 5 but can be changed.\n\nThis function uses Random Forest Classifier as model. In the default code, there are some modification to the parameters in trying to increase model’s success. \n\nThe function returns the performance of the model in different training models to help the user evaluate the success. The output informs the user about the overfitting and the score of the model (how accurately the model assigned the categories to the data points compared to actual categorisation).\n\n- do_cross_val(kf, df)\nThis function is nearly identical to cross_validate(df). User should check which one works the best for the visualisation of the model success. \n\n- trainParameters(df)\nThis function trains parameters of the model to optimise them. Default code deals with max_depth and min_samples_leaf parameters in Random Forest Classifier for smoothing. User can add additional parameters available to Random Forest Classifier.\n\n","metadata":{},"cell_type":"markdown","id":"961bb533-febb-4b9d-b0a5-56b08184a436"},{"source":"# Encoding the Primary Topic column\ntopicMap = {\n    'Demand impact' : 0,\n    'People' : 1,\n    'Policy recommendations' : 2,\n    'Supply impact' : 3,\n    'Other operational impact': 4\n}\ndf['label'] = df['Primary Topic'].map(topicMap)\n\nprint(\"Data loaded\")\n\n\n# Gives a more reliable score than just using one split (like below with train_test_split)\ndef cross_validate(df):\n    kf = StratifiedKFold(n_splits=5, shuffle=True) # idea - also try StratifiedKFold\n    score = 0\n    overfitScore = 0\n    count = 0\n\n    for trainIndices, testIndices in kf.split(df['wordVec'], df['label']):\n        train, test = df.iloc[trainIndices], df.iloc[testIndices]\n        X_train = np.array(train['wordVec'].tolist())\n        X_test = np.array(test['wordVec'].tolist())\n        y_train = train['label']\n        y_test = test['label']\n\n        print('Training model ' + str(count))\n\n\n        # Modify parameters to improve model/avoid overfitting\n        # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n        # min_samples_leaf : int -- recommended for smoothing\n        rf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=1234)\n\n\n        \n        rf_model = rf.fit(X_train, y_train.values)\n        print('Model ' + str(count) + ' fit')\n        score += rf.score(X_test, y_test)\n        overfitScore += rf.score(X_train, y_train)\n        print('Model ' + str(count) + ' scored')\n        count += 1\n\n    print(\"Score on training data (detect overfitting): \" + str(overfitScore/count))\n    return score/count\n\n\n\n\n# kf = KFold splitter, df = data frame\ndef do_cross_val(kf, df, n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n    score = 0\n    overfitScore = 0\n    count = 0\n            \n    for trainIndices, testIndices in kf.split(df['wordVec'], df['label']):\n        train, test = df.iloc[trainIndices], df.iloc[testIndices]\n        X_train = np.array(train['wordVec'].tolist())\n        X_test = np.array(test['wordVec'].tolist())\n        y_train = train['label']\n        y_test = test['label']\n\n\n        # Modify parameters to improve model/avoid overfitting\n        # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n        # min_samples_leaf : int -- recommended for smoothing\n         # rf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=1234)\n        rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=1234)\n\n        rf_model = rf.fit(X_train, y_train.values)\n        score += rf.score(X_test, y_test)\n        overfitScore += rf.score(X_train, y_train)\n        count += 1\n    \n    return score/count, overfitScore/count\n\ndef trainParameters(df):\n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234) # idea - also try StratifiedKFold\n\n    min_samples_leaf_values = [1, 2, 4, 8, 16, 32]\n    max_depth = [None, 15, 12, 9, 6]\n    \n    results = []\n    overfitResults = []\n    \n    for leaf_samples in min_samples_leaf_values:\n        leafResults = []\n        overfitLeafResults = []\n        \n        for depth in max_depth:\n            score, overfitScore = do_cross_val(kf, df, max_depth=depth, min_samples_leaf=leaf_samples)\n            \n\n            print('min_samples_leaf: ' + str(leaf_samples) + ', max_depth: ' + str(depth) + ', score: ' + str(score) + ', overfit score: ' + str(overfitScore))\n            leafResults.append(score)\n            overfitLeafResults.append(overfitScore)\n\n        results.append(leafResults)\n        overfitResults.append(overfitLeafResults)\n\n    return min_samples_leaf_values, max_depth, results, overfitResults\n\n","metadata":{},"cell_type":"code","id":"ad04db02-709a-40af-a492-9a26192f2817","execution_count":null,"outputs":[]},{"source":"#### Displaying the output in excel format\n\n- saveDf(name)\nSaves the predictions of this model alongside the real classifications to inspect errors manually. \n","metadata":{},"cell_type":"markdown","id":"bdea6f4d-3ece-4710-9ae1-e93a7bfe4e25"},{"source":"def saveDf(name):\n    df.to_excel('C:\\\\Users\\\\username\\\\Downloads\\\\' + name)\n","metadata":{},"cell_type":"code","id":"7cd72a62-2ab0-4540-a82f-4065f2ae8a65","execution_count":null,"outputs":[]},{"source":"## Results and Observations\n\nThis model and all its variations have an average of 65.3% percent success rate in matching data points to their primary topic. The average error rate is above acceptable (34.7%) and this model is not yet fit to be used in classification of data points. \n\n","metadata":{},"cell_type":"markdown","id":"1dd484fd-cb3f-43ba-92ff-b36ce9830d64"},{"source":"## Further Suggestions\n\nMain problems regarding the data used that seemingly decrease the success of the model is as follows:\n\nGrammar: A considerable number of data points include bad grammar which makes the word vector coordinates of a data point less accurate and therefore decreases the quality of the classification. \nData points that are too short: When inspected, some of the data points turn out to be blank after stop word removal was applied which hindered the classification considerably.\nBlank data points: Some data points have no words in them which involuntarily trained the model to assign “supply impact” category for empty rows. \n","metadata":{},"cell_type":"markdown","id":"0bb8df28-fefb-43ff-8d23-e061abe8bb59"},{"source":"## Remarks, Notes and Further Improvment in Script","metadata":{},"cell_type":"markdown","id":"fec553f2-cb8e-46fd-89f3-14414ce1bf63"},{"source":"# random_state=1234 makes result reproducible\n##train, test = train_test_split(df, test_size=0.1, random_state=1)\n##\n##X_train = np.array(train['wordVec'].tolist())\n##X_test = np.array(test['wordVec'].tolist())\n##y_train = train['label']\n##y_test = test['label']\n##\n##print(\"Train/test split\")\n##\n### random_state=1234 makes result reproducible\n##rf = RandomForestClassifier(random_state=1234)\n##rf_model = rf.fit(X_train, y_train.values)\n##\n##print(\"Random Forest model fit\")\n##\n##print(rf.score(X_test, y_test))\n# 0.5982142857142857\n\n#print(cross_validate(df))\n\n#print(df.groupby('Primary Topic')['Primary Topic'].count() / len(df))\n\n#print(create_df())\n\n\n# Select single class vs probability split between classes\n# rf.predict(X_test)\n# rf.predict_proba(X_test)\n\n### Model training\n# varying min_samples_leaf_values or max_depth seemingly still only max 65%\n# TODO: Try filtering out rows with no words found before doing split\n","metadata":{},"cell_type":"code","id":"81d5dd57-700e-4668-95eb-2de5a1b51deb","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}
